---
title: "An Analysis of Climate Sentiment Across the USA"
author: "Xander Atalay, Zoe Pham, Umar Abushaban"
date: "10/20/2021"
output:
  html_document:
    toc: yes
    theme: cosmo
    toc_float: yes
  pdf_document:
    toc: yes
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE, cache = FALSE)
```

```{r Libraries, include=FALSE}
library(tidyverse)
library(tidytext)
library(ggwordcloud)
library(gutenbergr)
library(textdata)
```

## Introduction:

### Our group is taking a look at six different regions in the continental United States: Northeast, Northwest, Midwest, West, Southwest, and Southeast. Our goal is to analyze the sentiment surrounding climate change in those regions by using text mining algorithms on popular regional news sources. {.unlisted .unnumbered}

## Initial Exploration in Each Region: {.tabset}

### Boston Magazine:

```{r Northeast, echo = FALSE}
# We start by loading in the data, which is just a txt file containing all of the text from each journal.
BostonMagazineClimate <- tibble(text = read_lines('BostonMagazineClimate.txt'))
# 29 articles from the Boston Magazine were used for this analysis, amounting to approximately 107,371 words.

# Seperating out the words into individual rows.
BM_Words <-  BostonMagazineClimate %>% unnest_tokens(word, text)
# removing any stop words.
BM_Words_SW <- BM_Words %>% anti_join(stop_words)
# After removing the stop words, we can see that the total word count has halved.
# Next, we count the frequency of each word in the corpus.
BM_Count <- BM_Words_SW %>% count(word, sort=TRUE)
# We can take a look at this table to get a sense of the most common words in this article. Unsuprisingly, it is very... Boston.

# Next, we'll start with the sentiment analysis. We use three different sentiment data sets: affin, nrc, and bing.
BMSentiment_affin <- BM_Words %>%
  inner_join(get_sentiments("afinn"))

BMSentiment_nrc <- BM_Words %>%
  inner_join(get_sentiments("nrc"))

BMSentiment_bing <- BM_Words %>%
  inner_join(get_sentiments("bing"))

# Taking a look at the bing sentiment analysis, which just gives us a sense of the ratio between positive and negative words.
cat("Bing Sentiment Analysis:")
table(BMSentiment_bing$sentiment)
# We can compare the ratios of positive to negative words in different regions.
cat(paste("Positive Word to Negative Word Ratio:", 
          as.character(round((3266/2114),digits = 2))))


# The nrc sentiment analysis is more complex, and ranks words based on different characteristics beyond positive and negative.
cat("NRC Sentiment Analysis")
table(BMSentiment_nrc$sentiment)
# Creating a histogram of those words
(ggplot(BMSentiment_nrc, aes(x = sentiment)) + 
  geom_bar())

# Finally, we can look at the affin sentiment (which ranks how positive a word is on a scale of -5 to 5) and determine the frequency of more positive words.
cat("Affin Sentiment Analysis")
(ggplot(data = BMSentiment_affin, 
       aes(x=value))+
  geom_histogram()+
  ggtitle("Boston Magazine Climate Sentiment Range")+
  theme_minimal())

set.seed(42)
ggplot(BM_Count[1:50,], aes(label = word, size = n )) + geom_text_wordcloud(rm_outside = TRUE) + scale_size(range = c(2,12)) + theme_minimal()
```



### Chicago Daily Herald:
```{r Midwest, echo = FALSE}
# Now we repeat those steps with different regions, next up we're doing the midwest, using the Chicago Daily Herald as our Source.

# Loading in the data:
ChicagoHeraldClimate <- tibble(text = read_lines('ChicagoHeraldClimate.txt'))
# 80 articles from the Chicago Herald were used for this analysis, amounting to approximately 68,923 words.

# Seperating out the words into individual rows.
CH_Words <-  ChicagoHeraldClimate %>% unnest_tokens(word, text)
# removing any stop words.
CH_Words_SW <- CH_Words %>% anti_join(stop_words)
# After removing the stop words, we can see that the total word count has almost exactly halved.

# Next, we count the frequency of each word in the corpus.
CH_Count <- CH_Words_SW %>% count(word, sort=TRUE)
# We can take a look at this table to get a sense of the most common words in this article. We see climate and change at the top two, indicating that we have a good text selection for this analysis.

# Sentiment Analysis
CHSentiment_affin <- CH_Words %>%
  inner_join(get_sentiments("afinn"))

CHSentiment_nrc <- CH_Words %>%
  inner_join(get_sentiments("nrc"))

CHSentiment_bing <- CH_Words %>%
  inner_join(get_sentiments("bing"))

# Taking a look at the bing sentiment analysis, which just gives us a sense of the ratio between positive and negative words.
cat("Bing Sentiment Analysis:")
table(CHSentiment_bing$sentiment)
# We can compare the ratios of positive to negative words in different regions.
cat(paste("Positive Word to Negative Word Ratio:", 
          as.character(round((1661/1577),digits = 2))))
# We can see this this is a lower ratio, meaning that the conversation surrounding climate change has a more negative sentiment in the Chicago Daily Herald.

# The nrc sentiment analysis is more complex, and ranks words based on different characteristics beyond positive and negative.
cat("NRC Sentiment Analysis")
table(CHSentiment_nrc$sentiment)
# Creating histogram
(ggplot(CHSentiment_nrc, aes(x = sentiment)) + 
  geom_bar())

# Finally, we can look at the affin sentiment (which ranks how positive a word is on a scale of -5 to 5) and determine the frequency of more positive words.
cat("Affin Sentiment Analysis")
(ggplot(data = CHSentiment_affin, 
       aes(x=value))+
  geom_histogram()+
  ggtitle("Chicago Daily Herald Climate Sentiment Range")+
  theme_minimal())

set.seed(42)
ggplot(CH_Count[1:50,], aes(label = word, size = n )) + geom_text_wordcloud(rm_outside = TRUE) + scale_size(range = c(2,12)) + theme_minimal()
```

### The Santa Fe New Mexican:

```{r Southwest, echo = FALSE}
# We load in a txt file containing all of the text from The Santa Fe New Mexican as a tibble.
SantaFeNewMexicanClimate <- tibble(text = read_lines('The Santa Fe New Mexican.txt'))


# Separate the words into individual rows.
SFNM_Words <-  SantaFeNewMexicanClimate %>% unnest_tokens(word, text)
# Removing any stop words.
SFNM_Words_SW <- SFNM_Words %>% anti_join(stop_words)
# Counting the frequency of each word in the corpus (entire body of Santa Fe New Mexican articles I selected)
SFNM_Count <- SFNM_Words_SW %>% count(word, sort=TRUE)

# Sentiment analysis. We use three different sentiment data sets: affin, nrc, and bing.
SFNMSentiment_affin <- SFNM_Words %>%
  inner_join(get_sentiments("afinn"))

SFNMSentiment_nrc <- SFNM_Words %>%
  inner_join(get_sentiments("nrc"))

SFNMSentiment_bing <- SFNM_Words %>%
  inner_join(get_sentiments("bing"))

# Taking a look at the bing sentiment analysis, which just gives us a sense of the ratio between positive and negative words.
cat("Bing Sentiment Analysis:")
table(SFNMSentiment_bing$sentiment)
# We can compare the ratios of positive to negative words in different regions.
cat(paste("Positive Word to Negative Word Ratio:", 
          as.character(round((337/394),digits = 2))))


# The nrc sentiment analysis is more complex, and ranks words based on different characteristics beyond positive and negative.
cat("NRC Sentiment Analysis")
table(SFNMSentiment_nrc$sentiment)
ggplot(SFNMSentiment_nrc, aes(x = sentiment)) + geom_bar()

# Finally, we can look at the affin sentiment (which ranks how positive a word is on a scale of -5 to 5) and determine the frequency of more positive words.
cat("Affin Sentiment Analysis")
(ggplot(data = SFNMSentiment_affin, 
       aes(x=value))+
  geom_bar()+
  ggtitle("The Santa Fe New Mexican Sentiment Range")+
  theme_minimal())

# Word Cloud
set.seed(42)
ggplot(SFNM_Count[1:50,], aes(label = word, size = n )) + geom_text_wordcloud(rm_outside = TRUE) + scale_size(range = c(2,12)) + theme_minimal()
```

### Tampa Bay Times:
```{r Southeast, echo = FALSE}


# Loading in the data:
TampaBayTimesClimate <- tibble(text = read_lines('Tampa Bay Times.txt'))

# Seperating out the words into individual rows.
TBT_Words <-  TampaBayTimesClimate %>% unnest_tokens(word, text)
# removing any stop words.
TBT_Words_SW <- TBT_Words %>% anti_join(stop_words)
# After removing the stop words, we can see that the total word count has almost exactly halved.

# Next, we count the frequency of each word in the corpus.
TBT_Count <- TBT_Words_SW %>% count(word, sort=TRUE)


# Sentiment Analysis
TBTSentiment_affin <- TBT_Words %>%
  inner_join(get_sentiments("afinn"))

TBTSentiment_nrc <- TBT_Words %>%
  inner_join(get_sentiments("nrc"))

TBTSentiment_bing <- TBT_Words %>%
  inner_join(get_sentiments("bing"))

# Taking a look at the bing sentiment analysis, which just gives us a sense of the ratio between positive and negative words.
cat("Bing Sentiment Analysis:")
table(TBTSentiment_bing$sentiment)
# We can compare the ratios of positive to negative words in different regions.
cat(paste("Positive Word to Negative Word Ratio:", 
          as.character(round((392/382),digits = 2))))
# The positive to negative word ratio is higher in the Tampa Bay Times compared to the Santa Fe New Mexican, so the conversation surrounding climate change has a more positive sentiment in this region.

# The nrc sentiment analysis ranks words based on different characteristics beyond positive and negative.
cat("NRC Sentiment Analysis")
ggplot(TBTSentiment_nrc, aes(x = sentiment)) + geom_bar()
cat("Affin Sentiment Analysis")
(ggplot(data = TBTSentiment_affin, 
       aes(x=value))+
  geom_bar()+
  ggtitle("Tampa Bay Times Climate Sentiment Range")+
  theme_minimal())

# Word Cloud
set.seed(42)
ggplot(TBT_Count[1:50,], aes(label = word, size = n )) + geom_text_wordcloud(rm_outside = TRUE) + scale_size(range = c(2,12)) + theme_minimal()

```

### The Los Angeles Times:

```{r West, echo = FALSE}
# We load in a txt file containing all of the text from The LA Times as a tibble.
LATimesClimate <- tibble(text = read_lines('LA_Times_Climate.txt'))

# Separate the words into individual rows.
LA_Words <-  LATimesClimate %>% unnest_tokens(word, text)
# Removing any stop words.
LA_Words_SW <- LA_Words %>% anti_join(stop_words)
# Counting the frequency of each word in the corpus (entire body of the LA Times articles in the .txt file)
LA_Count <- LA_Words_SW %>% count(word, sort=TRUE)

# Sentiment analysis. We use three different sentiment data sets: affin, nrc, and bing.
LASentiment_affin <- LA_Words %>%
  inner_join(get_sentiments("afinn"))

LASentiment_nrc <- LA_Words %>%
  inner_join(get_sentiments("nrc"))

LASentiment_bing <- LA_Words %>%
  inner_join(get_sentiments("bing"))

# Taking a look at the bing sentiment analysis, which just gives us a sense of the ratio between positive and negative words.
cat("Bing Sentiment Analysis:")
table(LASentiment_bing$sentiment)
# We can compare the ratios of positive to negative words in different regions.
cat(paste("Positive Word to Negative Word Ratio:", 
          as.character(round((1082/763 ),digits = 2))))


# The nrc sentiment analysis is more complex, and ranks words based on different characteristics beyond positive and negative.
cat("NRC Sentiment Analysis")
table(LASentiment_nrc$sentiment)
ggplot(LASentiment_nrc, aes(x = sentiment)) + geom_bar()

# Finally, we can look at the affin sentiment (which ranks how positive a word is on a scale of -5 to 5) and determine the frequency of more positive words.
cat("Affin Sentiment Analysis")
(ggplot(data = LASentiment_affin, 
       aes(x=value))+
  geom_bar()+
  ggtitle("The Los Angeles Times Sentiment Range")+
  theme_minimal())

# Word Cloud
set.seed(42)
ggplot(LA_Count[1:50,], aes(label = word, size = n )) + geom_text_wordcloud(rm_outside = TRUE) + scale_size(range = c(2,12)) + theme_minimal()
```

### The Seattle Times
```{r Northwest, echo = FALSE}




# Loading in the data:
SeattleTimesClimate <- tibble(text = read_lines('Seattle_Times_Climate.txt'))

# Seperating out the words into individual rows.
Seattle_Words <-  SeattleTimesClimate %>% unnest_tokens(word, text)
# removing any stop words.
Seattle_Words_SW <- Seattle_Words %>% anti_join(stop_words)
# After removing the stop words, we can see that the total word count has almost exactly halved.

# Next, we count the frequency of each word in the corpus.
Seattle_Count <- Seattle_Words_SW %>% count(word, sort=TRUE)


# Sentiment Analysis
SeattleSentiment_affin <- Seattle_Words %>%
  inner_join(get_sentiments("afinn"))

SeattleSentiment_nrc <- Seattle_Words %>%
  inner_join(get_sentiments("nrc"))

SeattleSentiment_bing <- Seattle_Words %>%
  inner_join(get_sentiments("bing"))

# Taking a look at the bing sentiment analysis, which just gives us a sense of the ratio between positive and negative words.
cat("Bing Sentiment Analysis:")
table(SeattleSentiment_bing$sentiment)
# We can compare the ratios of positive to negative words in different regions.
cat(paste("Positive Word to Negative Word Ratio:", 
          as.character(round((577/547),digits = 2))))
# The positive to negative word ratio is higher in The Seattle Times compared to the Los Angeles Times, so the conversation surrounding climate change has a more positive sentiment in this region.

# The nrc sentiment analysis ranks words based on different characteristics beyond positive and negative.
cat("NRC Sentiment Analysis")
ggplot(SeattleSentiment_nrc, aes(x = sentiment)) + geom_bar()
cat("Affin Sentiment Analysis")
(ggplot(data = SeattleSentiment_affin, 
       aes(x=value))+
  geom_bar()+
  ggtitle("The Seattle Times Climate Sentiment Range")+
  theme_minimal())

# Word Cloud
set.seed(42)
ggplot(Seattle_Count[1:50,], aes(label = word, size = n )) + geom_text_wordcloud(rm_outside = TRUE) + scale_size(range = c(2,12)) + theme_minimal()

```





## Summarized Differences Between Climate Sentiment in Each Region:

```{r Comparison Plots}
# Setting Up a Stacked Bar Chart
Source <- c(rep("Boston" , 2) , rep("Chicago" , 2), rep("Tampa", 2), rep("New Mexico", 2), rep("Los Angeles", 2), rep("Seattle", 2))
Sentiment <- c("Positive" , "Negative")
Words <- c(3266,2114,1661,1577, 337,394, 392,382, 1082,763, 577,547)
Country <- data.frame(Source,Sentiment,Words)

ggplot(Country, aes(fill=Sentiment, y=Words, x=Source)) + 
  geom_bar(position="stack", stat="identity")

TextSource <- c("Boston", "Chicago", "Tampa", "New Mexico", "Los Angeles", "Seattle")
PositiveWordRatio <- c(1.54,1.05, 1.03, 0.86, 1.42, 1.05)
BingRatios <- data.frame(TextSource,PositiveWordRatio)
BingRatios

(ggplot(data = BingRatios, aes(x = TextSource, y = PositiveWordRatio, fill = PositiveWordRatio)) + 
  geom_bar(stat = "identity"))
```
The positive word ratio is the highest in Boston and Los Angeles, over 1.4, whereas Chicago, Seattle, and Tampa have lower positive word ratios under 1.2, the lowest ratio under 1.0 for New Mexico. This tells us that the Northeast and Northwest regions are more positive when discussing climate change, possibly due to public acknowledgement of climate change and willingness to talk about positive ways to combat it through public policy. 

## TF-IDF:

### Here is a table of the 25 words with the highest tf_idf values:
```{r}
# Transposing data alongside each other to compare vs on top of each other
data_prep <- function(x,y,z){
  i <- as_tibble(t(x))
  ii <- unite(i,"text",y:z,remove = TRUE,sep = " ")}

BM_bag <- data_prep(BostonMagazineClimate, 'V1', 'V3168') # 51873
CH_bag <- data_prep(ChicagoHeraldClimate, 'V1','V2441')
TBT_bag <- data_prep(TampaBayTimesClimate, 'V1','V1275')
SFNM_bag <- data_prep(SantaFeNewMexicanClimate, 'V1', 'V987')
LA_bag <- data_prep(LATimesClimate, 'V1', 'V1377')
Seattle_bag <- data_prep(SeattleTimesClimate, 'V1', 'V1316')

regions <- c("Northeast","Midwest","Southwest", "Southeast", "West", "Northwest")

tf_idf_text <- tibble(regions,text=t(tibble(BM_bag,CH_bag, TBT_bag, SFNM_bag, LA_bag, Seattle_bag, .name_repair = "universal")))

View(tf_idf_text)

word_count <- tf_idf_text %>%
  unnest_tokens(word, text) %>%
  count(regions, word, sort = TRUE)

total_words <- word_count %>%
  group_by(regions) %>%
  summarize(total = sum(n))

region_words <- left_join(word_count, total_words)

region_words <- region_words %>%
  bind_tf_idf(word, regions, n)

#usually look at the top 25ish words and see if there are pattern differences

region_words <- region_words[order(region_words$tf_idf, decreasing = TRUE),]

print.data.frame(head(region_words, 25))


# term frequency - inverse document frequency tf-idf. Here we are going to treat each of our speeches as a document in a corpus and explore the relative importance of words to these speeches as compared to the overall corpus.

#ratio of how often a term shows up in the whole corpus vs indiv doc

```



### To get a better sense of term frequency, we used the following statistic: {.unlisted .unnumbered}
### ((Specific word count per region)/(total specific word count))/(words per region/25000){.unlisted .unnumbered}

```{r}
BM_Merge <- BM_Count %>% rename(Boston = n)
CH_Merge <- CH_Count %>% rename(Chicago = n)
LA_Merge <- LA_Count %>% rename(LA = n)
SE_Merge <- Seattle_Count %>% rename(Seattle = n)
TBT_Merge <- TBT_Count %>% rename(Tampa_Bay = n)
SFNM_Merge <- SFNM_Count %>% rename(Santa_Fe = n)

TFTable <- merge(merge(merge(merge(merge(BM_Merge,
                                      CH_Merge, by = 'word'),
                                SFNM_Merge, by = 'word'),
                          TBT_Merge, by = 'word'),
                    LA_Merge, by = 'word'),
              SE_Merge, by = 'word')

TFTable$Total <- rowSums(TFTable[2:6])

TFTable <- TFTable[order(TFTable$Total, decreasing = TRUE),]
TFRelavent <- head(TFTable, 25)

range01 <- function(x){(x-min(x))/(max(x)-min(x))}

lengths <- c(length(BM_Words_SW$word),
             length(CH_Words_SW$word),
             length(SFNM_Words_SW$word), 
             length(TBT_Words_SW$word), 
             length(LA_Words_SW$word), 
             length(Seattle_Words_SW$word))

per2500 <- lengths/25000

for (i in 1:6){
  TFRelavent[,i+1] <- TFRelavent[,i+1]/TFRelavent$Total
}

TFScaled <- TFRelavent

for (i in 1:6){
  TFScaled[,i+1] <- TFRelavent[,i+1]/per2500[i]
}


TFScaled <- TFScaled[,1:7]
TFScaledInv = data.frame(t(TFScaled))
names(TFScaledInv) <- lapply(TFScaledInv[1, ], as.character)
TFScaledInv <- TFScaledInv[-1,] 

print(TFScaledInv)

TFScaledInv$City <- c("Boston", "Chicago", "Santa Fe", "Tampa Bay", "Los Angeles", "Seattle")

```

### Looking at a few specific words in more depth:{.unlisted .unnumbered}

```{r}

ggplot(data = TFScaledInv, aes(x = City, y = water, fill = water)) +
  geom_bar(stat = "identity") + 
  scale_fill_brewer(palette="Blues") + 
  theme_minimal()

ggplot(data = TFScaledInv, aes(x = City, y = heat, fill = heat)) +
  geom_bar(stat = "identity") + 
  scale_fill_brewer(palette="Oranges") + 
  theme_minimal()

ggplot(data = TFScaledInv, aes(x = City, y = energy, fill = energy)) +
  geom_bar(stat = "identity") + 
  scale_fill_brewer(palette="Greens") + 
  theme_minimal()

ggplot(data = TFScaledInv, aes(x = City, y = power, fill = power)) +
  geom_bar(stat = "identity") + 
  scale_fill_brewer(palette="Purples") + 
  theme_minimal()

ggplot(data = TFScaledInv, aes(x = City, y = air, fill = air)) +
  geom_bar(stat = "identity") + 
  scale_fill_brewer(palette="Greys") + 
  theme_minimal()

ggplot(data = TFScaledInv, aes(x = City, y = world, fill = world)) +
  geom_bar(stat = "identity") + 
  scale_fill_brewer(palette="Reds") + 
  theme_minimal()

ggplot(data = TFScaledInv, aes(x = City, y = president, fill = president)) +
  geom_bar(stat = "identity") + 
  scale_fill_brewer(palette="Spectral") + 
  theme_minimal()


```

Locations such as cities or state names, regional environmental features or residents affected by climate change such as ranchers or coral, and natural disasters such as hurricanes or drought have the highest tf-idf ratios, and are most important to their corpuses, or regional newspapers. This makes sense because each corpus or region is more concerned with their own residents and how climate change affects them.

Pattern differences between regions include differing concerns unique to each region like above, with more coastal or farming issues arising to corresponding geographic regions. 

Looking at the relative ratios between each region and the US as a whole, we see some interesting but expected results that depend on specific words.

## Policy Suggestions

### We don't get a ton of information from this project that would influence policy suggestions, but generally policies that we think would help reduce the effects of global warming are...{.unlisted .unnumbered}
