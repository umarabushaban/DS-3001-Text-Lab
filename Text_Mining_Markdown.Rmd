---
title: "An Analysis of Climate Sentiment Across the USA"
author: "Xander Atalay, Zoe Pham, Umar Abushaban"
date: "10/20/2021"
output:
  html_document:
    toc: yes
    theme: cosmo
    toc_float: yes
  pdf_document:
    toc: yes
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE, cache = FALSE)
```

```{r Libraries, include=FALSE}
library(tidyverse)
library(tidytext)
library(ggwordcloud)
library(gutenbergr)
library(textdata)
```

## Introduction:

### Our group is taking a look at six different regions in the continental United States: Northeast, Midwest ... Our goal is to analyze the sentiment surrounding climate change in those regions by using text mining algorithms on news sources in those regions. {.unlisted .unnumbered}

## Initial Exploration in Each Region: {.tabset}

### Boston Magazine:

```{r Northeast, echo = FALSE}
# We start by loading in the data, which is just a txt file containing all of the text from each journal.
BostonMagazineClimate <- tibble(text = read_lines('BostonMagazineClimate.txt'))
# 29 articles from the Boston Magazine were used for this analysis, amounting to approximately 107,371 words.

# Seperating out the words into individual rows.
BM_Words <-  BostonMagazineClimate %>% unnest_tokens(word, text)
# removing any stop words.
BM_Words_SW <- BM_Words %>% anti_join(stop_words)
# After removing the stop words, we can see that the total word count has halved.
# Next, we count the frequency of each word in the corpus.
BM_Count <- BM_Words_SW %>% count(word, sort=TRUE)
# We can take a look at this table to get a sense of the most common words in this article. Unsuprisingly, it is very... Boston.

# Next, we'll start with the sentiment analysis. We use three different sentiment data sets: affin, nrc, and bing.
BMSentiment_affin <- BM_Words %>%
  inner_join(get_sentiments("afinn"))

BMSentiment_nrc <- BM_Words %>%
  inner_join(get_sentiments("nrc"))

BMSentiment_bing <- BM_Words %>%
  inner_join(get_sentiments("bing"))

# Taking a look at the bing sentiment analysis, which just gives us a sense of the ratio between positive and negative words.
cat("Bing Sentiment Analysis:")
table(BMSentiment_bing$sentiment)
# We can compare the ratios of positive to negative words in different regions.
cat(paste("Positive Word to Negative Word Ratio:", 
          as.character(round((3266/2114),digits = 2))))


# The nrc sentiment analysis is more complex, and ranks words based on different characteristics beyond positive and negative.
cat("NRC Sentiment Analysis")
table(BMSentiment_nrc$sentiment)
# Creating a histogram of those words
(ggplot(BMSentiment_nrc, aes(x = sentiment)) + 
  geom_bar())

# Finally, we can look at the affin sentiment (which ranks how positive a word is on a scale of -5 to 5) and determine the frequency of more positive words.
cat("Affin Sentiment Analysis")
(ggplot(data = BMSentiment_affin, 
       aes(x=value))+
  geom_histogram()+
  ggtitle("Boston Magazine Climate Sentiment Range")+
  theme_minimal())

set.seed(42)
ggplot(BM_Count[1:50,], aes(label = word, size = n )) + geom_text_wordcloud(rm_outside = TRUE) + scale_size(range = c(2,12)) + theme_minimal()
```



### Chicago Daily Herald:
```{r Midwest, echo = FALSE}
# Now we repeat those steps with different regions, next up we're doing the midwest, using the Chicago Daily Herald as our Source.

# Loading in the data:
ChicagoHeraldClimate <- tibble(text = read_lines('ChicagoHeraldClimate.txt'))
# 80 articles from the Chicago Herald were used for this analysis, amounting to approximately 68,923 words.

# Seperating out the words into individual rows.
CH_Words <-  ChicagoHeraldClimate %>% unnest_tokens(word, text)
# removing any stop words.
CH_Words_SW <- CH_Words %>% anti_join(stop_words)
# After removing the stop words, we can see that the total word count has almost exactly halved.

# Next, we count the frequency of each word in the corpus.
CH_Count <- CH_Words_SW %>% count(word, sort=TRUE)
# We can take a look at this table to get a sense of the most common words in this article. We see climate and change at the top two, indicating that we have a good text selection for this analysis.

# Sentiment Analysis
CHSentiment_affin <- CH_Words %>%
  inner_join(get_sentiments("afinn"))

CHSentiment_nrc <- CH_Words %>%
  inner_join(get_sentiments("nrc"))

CHSentiment_bing <- CH_Words %>%
  inner_join(get_sentiments("bing"))

# Taking a look at the bing sentiment analysis, which just gives us a sense of the ratio between positive and negative words.
cat("Bing Sentiment Analysis:")
table(CHSentiment_bing$sentiment)
# We can compare the ratios of positive to negative words in different regions.
cat(paste("Positive Word to Negative Word Ratio:", 
          as.character(round((1661/1577),digits = 2))))
# We can see this this is a lower ratio, meaning that the conversation surrounding climate change has a more negative sentiment in the Chicago Daily Herald.

# The nrc sentiment analysis is more complex, and ranks words based on different characteristics beyond positive and negative.
cat("NRC Sentiment Analysis")
table(CHSentiment_nrc$sentiment)
# Creating histogram
(ggplot(CHSentiment_nrc, aes(x = sentiment)) + 
  geom_bar())

# Finally, we can look at the affin sentiment (which ranks how positive a word is on a scale of -5 to 5) and determine the frequency of more positive words.
cat("Affin Sentiment Analysis")
(ggplot(data = CHSentiment_affin, 
       aes(x=value))+
  geom_histogram()+
  ggtitle("Chicago Daily Herald Climate Sentiment Range")+
  theme_minimal())

set.seed(42)
ggplot(CH_Count[1:50,], aes(label = word, size = n )) + geom_text_wordcloud(rm_outside = TRUE) + scale_size(range = c(2,12)) + theme_minimal()
```

### The Santa Fe New Mexican:

```{r Southwest, echo = FALSE}
# We load in a txt file containing all of the text from The Santa Fe New Mexican as a tibble.
SantaFeNewMexicanClimate <- tibble(text = read_lines('The Santa Fe New Mexican.txt'))


# Separate the words into individual rows.
SFNM_Words <-  SantaFeNewMexicanClimate %>% unnest_tokens(word, text)
# Removing any stop words.
SFNM_Words_SW <- SFNM_Words %>% anti_join(stop_words)
# Counting the frequency of each word in the corpus (entire body of Santa Fe New Mexican articles I selected)
SFNM_Count <- SFNM_Words_SW %>% count(word, sort=TRUE)

# Sentiment analysis. We use three different sentiment data sets: affin, nrc, and bing.
SFNMSentiment_affin <- SFNM_Words %>%
  inner_join(get_sentiments("afinn"))

SFNMSentiment_nrc <- SFNM_Words %>%
  inner_join(get_sentiments("nrc"))

SFNMSentiment_bing <- SFNM_Words %>%
  inner_join(get_sentiments("bing"))

# Taking a look at the bing sentiment analysis, which just gives us a sense of the ratio between positive and negative words.
cat("Bing Sentiment Analysis:")
table(SFNMSentiment_bing$sentiment)
# We can compare the ratios of positive to negative words in different regions.
cat(paste("Positive Word to Negative Word Ratio:", 
          as.character(round((337/394),digits = 2))))


# The nrc sentiment analysis is more complex, and ranks words based on different characteristics beyond positive and negative.
cat("NRC Sentiment Analysis")
table(SFNMSentiment_nrc$sentiment)
ggplot(SFNMSentiment_nrc, aes(x = sentiment)) + geom_bar()

# Finally, we can look at the affin sentiment (which ranks how positive a word is on a scale of -5 to 5) and determine the frequency of more positive words.
cat("Affin Sentiment Analysis")
(ggplot(data = SFNMSentiment_affin, 
       aes(x=value))+
  geom_bar()+
  ggtitle("The Santa Fe New Mexican Sentiment Range")+
  theme_minimal())

# Word Cloud
set.seed(42)
ggplot(SFNM_Count[1:50,], aes(label = word, size = n )) + geom_text_wordcloud(rm_outside = TRUE) + scale_size(range = c(2,12)) + theme_minimal()
```

### Tampa Bay Times:
```{r Southeast, echo = FALSE}


# Loading in the data:
TampaBayTimesClimate <- tibble(text = read_lines('Tampa Bay Times.txt'))

# Seperating out the words into individual rows.
TBT_Words <-  TampaBayTimesClimate %>% unnest_tokens(word, text)
# removing any stop words.
TBT_Words_SW <- TBT_Words %>% anti_join(stop_words)
# After removing the stop words, we can see that the total word count has almost exactly halved.

# Next, we count the frequency of each word in the corpus.
TBT_Count <- TBT_Words_SW %>% count(word, sort=TRUE)


# Sentiment Analysis
TBTSentiment_affin <- TBT_Words %>%
  inner_join(get_sentiments("afinn"))

TBTSentiment_nrc <- TBT_Words %>%
  inner_join(get_sentiments("nrc"))

TBTSentiment_bing <- TBT_Words %>%
  inner_join(get_sentiments("bing"))

# Taking a look at the bing sentiment analysis, which just gives us a sense of the ratio between positive and negative words.
cat("Bing Sentiment Analysis:")
table(TBTSentiment_bing$sentiment)
# We can compare the ratios of positive to negative words in different regions.
cat(paste("Positive Word to Negative Word Ratio:", 
          as.character(round((392/382),digits = 2))))
# The positive to negative word ratio is higher in the Tampa Bay Times compared to the Santa Fe New Mexican, so the conversation surrounding climate change has a more positive sentiment in this region.

# The nrc sentiment analysis ranks words based on different characteristics beyond positive and negative.
cat("NRC Sentiment Analysis")
ggplot(TBTSentiment_nrc, aes(x = sentiment)) + geom_bar()
cat("Affin Sentiment Analysis")
(ggplot(data = TBTSentiment_affin, 
       aes(x=value))+
  geom_bar()+
  ggtitle("Tampa Bay Times Climate Sentiment Range")+
  theme_minimal())

# Word Cloud
set.seed(42)
ggplot(TBT_Count[1:50,], aes(label = word, size = n )) + geom_text_wordcloud(rm_outside = TRUE) + scale_size(range = c(2,12)) + theme_minimal()

```




## Summarized Differences Between Climate Sentiment in Each Region:

```{r Comparison Plots}
# Setting Up a Stacked Bar Chart
Source <- c(rep("Boston" , 2) , rep("Chicago" , 2), rep("Tampa", 2), rep("New Mexico", 2))
Sentiment <- c("Positive" , "Negative")
Words <- c(3266,2114,1661,1577, 337,394, 392,382)
Country <- data.frame(Source,Sentiment,Words)

ggplot(Country, aes(fill=Sentiment, y=Words, x=Source)) + 
  geom_bar(position="stack", stat="identity")

TextSource <- c("Boston", "Chicago", "Tampa", "New Mexico")
PositiveWordRatio <- c(1.54,1.05, 1.03, 0.86 )
BingRatios <- data.frame(TextSource,PositiveWordRatio)
BingRatios

(ggplot(data = BingRatios, aes(x = TextSource, y = PositiveWordRatio, fill = PositiveWordRatio)) + 
  geom_bar(stat = "identity"))
```

### The Los Angeles Times:

```{r West, echo = FALSE}
# We load in a txt file containing all of the text from The LA Times as a tibble.
LATimesClimate <- tibble(text = read_lines('LA_Times_Climate.txt'))

# Separate the words into individual rows.
LA_Words <-  LATimesClimate %>% unnest_tokens(word, text)
# Removing any stop words.
LA_Words_SW <- LA_Words %>% anti_join(stop_words)
# Counting the frequency of each word in the corpus (entire body of the LA Times articles in the .txt file)
LA_Count <- LA_Words_SW %>% count(word, sort=TRUE)

# Sentiment analysis. We use three different sentiment data sets: affin, nrc, and bing.
LASentiment_affin <- LA_Words %>%
  inner_join(get_sentiments("afinn"))

LASentiment_nrc <- LA_Words %>%
  inner_join(get_sentiments("nrc"))

LASentiment_bing <- LA_Words %>%
  inner_join(get_sentiments("bing"))

# Taking a look at the bing sentiment analysis, which just gives us a sense of the ratio between positive and negative words.
cat("Bing Sentiment Analysis:")
table(LASentiment_bing$sentiment)
# We can compare the ratios of positive to negative words in different regions.
cat(paste("Positive Word to Negative Word Ratio:", 
          as.character(round((1082/763 ),digits = 2))))


# The nrc sentiment analysis is more complex, and ranks words based on different characteristics beyond positive and negative.
cat("NRC Sentiment Analysis")
table(LASentiment_nrc$sentiment)
ggplot(LASentiment_nrc, aes(x = sentiment)) + geom_bar()

# Finally, we can look at the affin sentiment (which ranks how positive a word is on a scale of -5 to 5) and determine the frequency of more positive words.
cat("Affin Sentiment Analysis")
(ggplot(data = LASentiment_affin, 
       aes(x=value))+
  geom_bar()+
  ggtitle("The Los Angeles Times Sentiment Range")+
  theme_minimal())

# Word Cloud
set.seed(42)
ggplot(LA_Count[1:50,], aes(label = word, size = n )) + geom_text_wordcloud(rm_outside = TRUE) + scale_size(range = c(2,12)) + theme_minimal()
```

### The Seattle Times
```{r Northwest, echo = FALSE}




# Loading in the data:
SeattleTimesClimate <- tibble(text = read_lines('Seattle_Times_Climate.txt'))

# Seperating out the words into individual rows.
Seattle_Words <-  SeattleTimesClimate %>% unnest_tokens(word, text)
# removing any stop words.
Seattle_Words_SW <- Seattle_Words %>% anti_join(stop_words)
# After removing the stop words, we can see that the total word count has almost exactly halved.

# Next, we count the frequency of each word in the corpus.
Seattle_Count <- Seattle_Words_SW %>% count(word, sort=TRUE)


# Sentiment Analysis
SeattleSentiment_affin <- Seattle_Words %>%
  inner_join(get_sentiments("afinn"))

SeattleSentiment_nrc <- Seattle_Words %>%
  inner_join(get_sentiments("nrc"))

SeattleSentiment_bing <- Seattle_Words %>%
  inner_join(get_sentiments("bing"))

# Taking a look at the bing sentiment analysis, which just gives us a sense of the ratio between positive and negative words.
cat("Bing Sentiment Analysis:")
table(SeattleSentiment_bing$sentiment)
# We can compare the ratios of positive to negative words in different regions.
cat(paste("Positive Word to Negative Word Ratio:", 
          as.character(round((577/547),digits = 2))))
# The positive to negative word ratio is higher in The Seattle Times compared to the Los Angeles Times, so the conversation surrounding climate change has a more positive sentiment in this region.

# The nrc sentiment analysis ranks words based on different characteristics beyond positive and negative.
cat("NRC Sentiment Analysis")
ggplot(SeattleSentiment_nrc, aes(x = sentiment)) + geom_bar()
cat("Affin Sentiment Analysis")
(ggplot(data = SeattleSentiment_affin, 
       aes(x=value))+
  geom_bar()+
  ggtitle("The Seattle Times Climate Sentiment Range")+
  theme_minimal())

# Word Cloud
set.seed(42)
ggplot(Seattle_Count[1:50,], aes(label = word, size = n )) + geom_text_wordcloud(rm_outside = TRUE) + scale_size(range = c(2,12)) + theme_minimal()

```

## TF-IDF:
```{r}
# Transposing data alongside each other to compare vs on top of each other 
data_prep <- function(x,y,z){
  i <- as_tibble(t(x))
  ii <- unite(i,"text",y:z,remove = TRUE,sep = " ")}
BM_bag <- data_prep(BM_Words, 'V1', 'V107371') # 51873
CH_bag <- data_prep(CH_Words, 'V1','V68923')
TBT_bag <- data_prep(TBT_Words, 'V1','V18946')
SFNM_bag <- data_prep(SFNM_Words, 'V1', 'V14141')
LA_bag <- data_prep(LA_Words, 'V1', 'V35993')
Seattle_bag <- data_prep(Seattle_Words, 'V1', 'V12650')

regions <- c("Northeast","Midwest","Southwest", "Southeast", "West", "Northwest")

tf_idf_text <- tibble(regions,text=t(tibble(BM_bag,CH_bag, TBT_bag, SFNM_bag,.name_repair = "universal")))

View(tf_idf_text)

word_count <- tf_idf_text %>%
  unnest_tokens(word, text) %>%
  count(regions, word, sort = TRUE)

total_words <- word_count %>% 
  group_by(regions) %>% 
  summarize(total = sum(n))

region_words <- left_join(word_count, total_words)

View(region_words)

region_words <- region_words %>%
  bind_tf_idf(word, regions, n)

#usually look at the top 25ish words and see if there are pattern differences

# term frequency - inverse document frequency tf-idf. Here we are going to treat each of our speeches as a document in a corpus and explore the relative importance of words to these speeches as compared to the overall corpus. 

#ratio of how often a term shows up in the whole corpus vs indiv doc

```

